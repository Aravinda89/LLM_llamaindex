{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0164ff46-208a-4a14-99de-932cc834932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6da766e-2863-4e61-a7cc-4bbcd12cb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad9f7f6-a716-465e-be8a-50213746fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.complete(\"Who is Paul Graham? in simple sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94b87e4-aff2-422e-9c58-3290226cecf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a British-American computer scientist, entrepreneur, and writer who co-founded Y Combinator, a renowned startup accelerator.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e69aa2-8f06-4fb2-a870-d1f6f787f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca96d1d-110a-4311-81d6-eed3fde18b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Me hearty! Me name be Captain Zazu P. Fizzypop, the most infamous pirate to ever sail the seven seas! Arrr!\n",
      "\n",
      "Me crew knows me as a swashbucklin' scoundrel with a heart o' gold and a penchant for gettin' out o' sticky situations with me quick wit and cunning. Me trusty parrot, Polly, be me constant companion, squawkin' advice and insults in equal measure.\n",
      "\n",
      "Now, what brings ye to these fair waters? Are ye lookin' for treasure, or just wantin' a taste o' pirate life?\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5030c9-9779-4c1b-b44f-27ffe290cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.stream_complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babfb099-9e29-46b8-ab05-4b381f52a976",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mdelta, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF_env\\lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:444\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen\u001b[1;34m()\u001b[0m\n\u001b[0;32m    442\u001b[0m last_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f_return_val:\n\u001b[0;32m    445\u001b[0m         dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    446\u001b[0m             LLMCompletionInProgressEvent(\n\u001b[0;32m    447\u001b[0m                 prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    450\u001b[0m             )\n\u001b[0;32m    451\u001b[0m         )\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cast(CompletionResponse, x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF_env\\lib\\site-packages\\llama_index\\core\\base\\llms\\generic_utils.py:124\u001b[0m, in \u001b[0;36mstream_chat_response_to_completion_response.<locals>.gen\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponseGen:\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m chat_response_gen:\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m CompletionResponse(\n\u001b[0;32m    126\u001b[0m             text\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    127\u001b[0m             additional_kwargs\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[0;32m    128\u001b[0m             delta\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mdelta,\n\u001b[0;32m    129\u001b[0m             raw\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mraw,\n\u001b[0;32m    130\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF_env\\lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:186\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m last_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f_return_val:\n\u001b[0;32m    187\u001b[0m         dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    188\u001b[0m             LLMChatInProgressEvent(\n\u001b[0;32m    189\u001b[0m                 messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m             )\n\u001b[0;32m    193\u001b[0m         )\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cast(ChatResponse, x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF_env\\lib\\site-packages\\llama_index\\llms\\ollama\\base.py:342\u001b[0m, in \u001b[0;36mOllama.stream_chat.<locals>.gen\u001b[1;34m()\u001b[0m\n\u001b[0;32m    339\u001b[0m response_txt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    341\u001b[0m tool_calls \u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m--> 342\u001b[0m token_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response_token_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_counts:\n\u001b[0;32m    344\u001b[0m     r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_counts\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF_env\\lib\\site-packages\\llama_index\\llms\\ollama\\base.py:203\u001b[0m, in \u001b[0;36mOllama._get_response_token_counts\u001b[1;34m(self, raw_response)\u001b[0m\n\u001b[0;32m    201\u001b[0m     prompt_tokens \u001b[38;5;241m=\u001b[39m raw_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_eval_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    202\u001b[0m     completion_tokens \u001b[38;5;241m=\u001b[39m raw_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 203\u001b[0m     total_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompletion_tokens\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7054f771-6b9e-49fc-a1c4-2b0973332de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2541a1ff-63ae-4351-b152-260c7d32bca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m resp:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mdelta, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF_env\\lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:186\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m last_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f_return_val:\n\u001b[0;32m    187\u001b[0m         dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    188\u001b[0m             LLMChatInProgressEvent(\n\u001b[0;32m    189\u001b[0m                 messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m             )\n\u001b[0;32m    193\u001b[0m         )\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cast(ChatResponse, x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF_env\\lib\\site-packages\\llama_index\\llms\\ollama\\base.py:342\u001b[0m, in \u001b[0;36mOllama.stream_chat.<locals>.gen\u001b[1;34m()\u001b[0m\n\u001b[0;32m    339\u001b[0m response_txt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    341\u001b[0m tool_calls \u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m--> 342\u001b[0m token_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response_token_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_counts:\n\u001b[0;32m    344\u001b[0m     r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_counts\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TF_env\\lib\\site-packages\\llama_index\\llms\\ollama\\base.py:203\u001b[0m, in \u001b[0;36mOllama._get_response_token_counts\u001b[1;34m(self, raw_response)\u001b[0m\n\u001b[0;32m    201\u001b[0m     prompt_tokens \u001b[38;5;241m=\u001b[39m raw_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_eval_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    202\u001b[0m     completion_tokens \u001b[38;5;241m=\u001b[39m raw_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 203\u001b[0m     total_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompletion_tokens\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806719b-8cca-4a08-bee5-26405e3e984a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
